{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at PAGE1\n",
      "--------------------PAGE 1 START--------------------\n",
      "--------------------PAGE 1 FINISH--------------------\n",
      "-TIME USED:-91.9789998531-\n",
      "Exported to csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "############################################################################################\n",
    "#Initial variables\n",
    "#This part of the code is used to set the inital parameters for the web crawler\n",
    "############################################################################################\n",
    "START_PAGE = 1\n",
    "END_PAGE = 1\n",
    "\n",
    "#For file naming\n",
    "PROJECT_ABBR = \"times\"\n",
    "PROJECT_NAME = \"Project 5\"\n",
    "\n",
    "#Base URL\n",
    "#Note:\n",
    "#the url below contain a parameter -1 that all records will be shown in one page thus it is not neccessary to turn to next page and so on.\n",
    "URL = \"https://www.timeshighereducation.com/world-university-rankings/2017/world-ranking#!/page/0/length/-1/sort_by/rank/sort_order/asc/cols/statss\"\n",
    "\n",
    "#Main window\n",
    "driver =webdriver.Firefox()\n",
    "driver.get(URL)\n",
    "\n",
    "df_columns = ['Ranking',\n",
    "              'University Name',\n",
    "              'Country',\n",
    "              'No. of Students',\n",
    "              'Source']\n",
    "\n",
    "#Create new dataframe with a the fields above\n",
    "df_etf = pd.DataFrame(columns=df_columns)\n",
    "    \n",
    "############################################################################################\n",
    "#go_to_page\n",
    "#This function is control the program to go to the start page\n",
    "#The implementation is quite simple that we press a number of click of button 'next_page'\n",
    "#As we have set all records in one page, it not necessarily for to use this function\n",
    "############################################################################################\n",
    "def go_to_page(page):\n",
    "    if page != 1:\n",
    "        print 'GOING TO PAGE' + str(page)\n",
    "        driver.get('https://www.timeshighereducation.com/world-university-rankings/2017/world-ranking#!/page/'+str(page-1)+'/length/25/sort_by/rank/sort_order/asc/cols/statss')\n",
    "        time.sleep(1)\n",
    "        print 'ARRIVED PAGE' + str(page)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "############################################################################################\n",
    "#save_file\n",
    "#This function is to save the file, where the file will be named different according to different state\n",
    "############################################################################################\n",
    "def save_file(state, CURRENT_PAGE):\n",
    "    if state == 'backup':\n",
    "        filename = '['+ PROJECT_ABBR +'][Intrim Backup][' + str(CURRENT_PAGE) + ']' + PROJECT_NAME + ' PAGE ' + str(START_PAGE) + ' to PAGE ' + str(CURRENT_PAGE) + ' Records.csv'\n",
    "        df_etf.to_csv(filename, index=False,  encoding='utf-8')\n",
    "        print '[Intrim Backup]Saved First '+ str(CURRENT_PAGE) + 'Pages Records to CSV'\n",
    "    elif state == 'all':\n",
    "        filename = '['+ PROJECT_ABBR +']'+ PROJECT_NAME +' PAGE ' + str(START_PAGE) + 'to PAGE ' + str(END_PAGE) + 'Records.csv'\n",
    "        df_etf.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print 'Exported to csv'\n",
    "\n",
    "#As we have set all records in one page, it not necessarily for to use this function\n",
    "#Turn to next page\n",
    "def next_page():\n",
    "    #click next page\n",
    "    driver.find_element_by_xpath('//*[@id=\"datatable-1_next\"]/a').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "############################################################################################\n",
    "#Main\n",
    "#This part is the main function of the program\n",
    "############################################################################################\n",
    "def main():\n",
    "    #Ensure the page is completely load            \n",
    "    time.sleep(2)\n",
    "                \n",
    "    #Go to the start page\n",
    "    go_to_page(START_PAGE)\n",
    "                \n",
    "    #Ensure the page is loaded after going to the destination page\n",
    "    time.sleep(2)\n",
    "                \n",
    "    print 'Start at PAGE'+ str(START_PAGE)\n",
    "    \n",
    "    #n in the row index for the dataframe\n",
    "    n = 1\n",
    "    \n",
    "    #This is a loop to loop over every page of the website\n",
    "    for CURRENT_PAGE in range(START_PAGE,END_PAGE+1):\n",
    "        print '--------------------PAGE ' + str(CURRENT_PAGE) +' START--------------------'\n",
    "        START_TIMESTAMP = time.time()\n",
    "        \n",
    "        #As all the <td> are wrapped by <tr>, we can use a loop to get the <td> inside a <td>\n",
    "        for row in driver.find_elements_by_xpath('//*[@id=\"datatable-1\"]/tbody/tr'):\n",
    "            #Extracting the fields ranking, university name, country and also no. of students\n",
    "            df_etf.loc[n, 'Ranking']         = row.find_element_by_xpath('td[1]').text\n",
    "            df_etf.loc[n, 'University Name'] = row.find_element_by_xpath('td[2]/a[1]').text\n",
    "            df_etf.loc[n, 'Country']         = row.find_element_by_xpath('td[2]/div/span').text\n",
    "            df_etf.loc[n, 'No. of Students'] = row.find_element_by_xpath('td[3]').text\n",
    "            n = n + 1\n",
    "        print '--------------------PAGE ' + str(CURRENT_PAGE) +' FINISH--------------------'\n",
    "        END_TIMESTAMP = time.time()\n",
    "        \n",
    "        #Print the time used for each page so as to estimate the remaining time of the program\n",
    "        print '-TIME USED:' + str(START_TIMESTAMP-END_TIMESTAMP) + '-'\n",
    "        \n",
    "        #store every first 5,10,15... pages to CSV for backup purpose\n",
    "        if(CURRENT_PAGE % 10) == 0:\n",
    "            save_file('backup', CURRENT_PAGE)\n",
    "                \n",
    "        #turn to next page(Not necessarily to use in this crawler)\n",
    "        next_page()\n",
    "        \n",
    "    #set Source as Times for every row as this is crawler for QS\n",
    "    df_etf['Source'] = PROJECT_ABBR\n",
    "    \n",
    "    #export the file to CSV\n",
    "    save_file('all', CURRENT_PAGE)\n",
    "    \n",
    "    #close the driver after the program finished\n",
    "    driver.quit()\n",
    "\n",
    "############################################################################################\n",
    "#This part is of the code is to tell the program to start function main() when it launches\n",
    "############################################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
